% Encoding: UTF-8

@inproceedings{Barrett_ICML_2008,
	Acmid = {1390162},
	Address = {New York, NY, USA},
	Author = {Barrett, Leon and Narayanan, Srini},
	Booktitle = {Proceedings of the 25th International Conference on Machine Learning},
	Date-Added = {2015-11-26 21:06:16 +0000},
	Date-Modified = {2015-11-26 21:06:28 +0000},
	Doi = {10.1145/1390156.1390162},
	Isbn = {978-1-60558-205-4},
	Location = {Helsinki, Finland},
	Numpages = {7},
	Pages = {41--47},
	Publisher = {ACM},
	Series = {ICML '08},
	Title = {Learning All Optimal Policies with Multiple Criteria},
	Url = {http://doi.acm.org/10.1145/1390156.1390162},
	Year = {2008},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1390156.1390162},
	Bdsk-Url-2 = {http://dx.doi.org/10.1145/1390156.1390162}}

@book{Bellman_PU_1957,
	Address = {Princeton, NJ},
	Author = {Bellman, Richard E.},
	Date-Added = {2015-11-12 07:47:16 +0000},
	Date-Modified = {2015-11-12 07:47:36 +0000},
	Keywords = {DP, Dynamic Programming},
	Publisher = {Princeton University Press},
	Title = {Dynamic {Programming}},
	Year = {1957}}

@book{Bertsekas_1987,
	Address = {Upper Saddle River, NJ, USA},
	Author = {Bertsekas, Dimitri P.},
	Date-Added = {2016-01-26 10:52:11 +0000},
	Date-Modified = {2016-01-26 10:52:11 +0000},
	Isbn = {0132215810},
	Publisher = {Prentice-Hall, Inc.},
	Title = {Dynamic Programming: Deterministic and Stochastic Models},
	Year = {1987}}

@article{Bertsimas_JFM_1998,
	Author = {Bertsimas, Dimitris and Lo, Andrew},
	Date-Added = {2015-11-26 16:52:34 +0000},
	Date-Modified = {2015-11-26 16:55:39 +0000},
	Journal = {Journal of Financial Markets},
	Number = {1},
	Pages = {1--50},
	Title = {Optimal control of execution costs},
	Volume = {1},
	Year = {1998},
	Bdsk-Url-1 = {http://EconPapers.repec.org/RePEc:eee:finmar:v:1:y:1998:i:1:p:1-50}}

@inproceedings{Boutilier_IJCAI_2001,
	Author = {Boutilier, Craig and Reiter, Ray and Price, Bob},
	Booktitle = {IJCAI},
	Date-Added = {2015-11-26 21:49:30 +0000},
	Date-Modified = {2015-11-26 21:49:30 +0000},
	Owner = {skinathil},
	Pages = {690--697},
	Timestamp = {2014.05.29},
	Title = {{Symbolic Dynamic Programming for First-order MDPs}},
	Year = {2001}} 



@article{Boutilier_JAIR_1999,
	Author = {Boutilier, Craig and Dean, Thomas and Hanks, Steve},
	Date-Added = {2016-01-26 23:44:12 +0000},
	Date-Modified = {2016-01-26 23:44:28 +0000},
	Journal = {Journal of Artificial Intelligence Research},
	Keywords = {DTP, MDP},
	Pages = {1--94},
	Title = {Decision-{Theoretic} {Planning}: {Structural} {Assumptions} and {Computational} {Leverage}},
	Volume = {11},
	Year = {1999}}

@incollection{Boyan_NIPS_2000,
	Author = {Justin A. Boyan and Michael L. Littman},
	Booktitle = {Advances in Neural Information Processing Systems 13},
	Date-Added = {2015-11-24 05:03:29 +0000},
	Date-Modified = {2015-11-24 05:04:07 +0000},
	Editor = {T.K. Leen and T.G. Dietterich and V. Tresp},
	Pages = {1026--1032},
	Publisher = {MIT Press},
	Title = {Exact Solutions to Time-Dependent MDPs},
	Url = {http://papers.nips.cc/paper/1811-exact-solutions-to-time-dependent-mdps.pdf},
	Year = {2001},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/1811-exact-solutions-to-time-dependent-mdps.pdf}}

@techreport{Chan_MIT_2001,
	Author = {Nicholas T. Chan and Christian R. Shelton},
	Date-Added = {2015-11-12 08:14:03 +0000},
	Date-Modified = {2015-11-12 08:14:15 +0000},
	Institution = {{MIT} {AI} Lab},
	Month = Apr,
	Number = {2001-005},
	Title = {An Electronic Market-Maker},
	Type = {AI Memo},
	Year = 2001}

@inproceedings{Dearden_UAI_1999,
	Acmid = {2073814},
	Address = {San Francisco, CA, USA},
	Author = {Dearden, Richard and Friedman, Nir and Andre, David},
	Booktitle = {Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence},
	Date-Added = {2016-01-26 10:54:08 +0000},
	Date-Modified = {2016-01-26 10:54:20 +0000},
	Isbn = {1-55860-614-9},
	Location = {Stockholm, Sweden},
	Numpages = {10},
	Pages = {150--159},
	Publisher = {Morgan Kaufmann Publishers Inc.},
	Series = {UAI'99},
	Title = {Model Based Bayesian Exploration},
	Url = {http://dl.acm.org/citation.cfm?id=2073796.2073814},
	Year = {1999},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=2073796.2073814}}

@Article{Givan_AI_2000,
  author =   {Robert Givan and Sonia Leach and Thomas Dean},
  title =    {Bounded-parameter Markov decision processes},
  journal =  {Artificial Intelligence },
  year =     {2000},
  volume =   {122},
  number =   {1â€“2},
  pages =    {71 - 109},
  doi =      {http://dx.doi.org/10.1016/S0004-3702(00)00047-3},
  issn =     {0004-3702},
  keywords = {Decision-theoretic planning},
  url =      {http://www.sciencedirect.com/science/article/pii/S0004370200000473}
}

@Article{Hopp_JOTA_1988,
  author =   {Hopp, W. J.},
  title =    {Sensitivity analysis in discrete dynamic programming},
  journal =  {Journal of Optimization Theory and Applications},
  year =     {1988},
  volume =   {56},
  number =   {2},
  pages =    {257--269},
  abstract = {The problem of characterizing the minimum perturbations to parameters in future stages of a discrete dynamic program necessary to change the optimal first policy is considered. Lower bounds on these perturbations are derived and used to establish ranges for the reward functions over which the optimal first policy is robust. A numerical example is presented to illustrate factors affecting the tightness of these bounds.},
  doi =      {10.1007/BF00939411},
  issn =     {1573-2878},
  url =      {http://dx.doi.org/10.1007/BF00939411}
}

@book{Howard_MIT_1960,
	Address = {Cambridge, Massachusetts, USA},
	Author = {Howard, Ronald Arthur},
	Date-Added = {2015-11-12 07:44:54 +0000},
	Date-Modified = {2015-11-12 07:48:18 +0000},
	Isbn = {0-262-08009-5 978-0-262-08009-5},
	Keywords = {DP, Dynamic Programming, MDP, Policy Iteration},
	Publisher = {The MIT press},
	Title = {Dynamic {Programming} and {Markov} {Processes}},
	Year = {1960}}

@article{Kaelbling_JoAIR_1998,
	Author = {Kaelbling, Leslie Pack and Littman, Michael L. and Cassandra, Anthony R.},
	Date-Added = {2015-11-10 11:33:12 +0000},
	Date-Modified = {2015-11-10 11:33:12 +0000},
	Doi = {http://dx.doi.org/10.1016/S0004-3702(98)00023-X},
	Issn = {0004-3702},
	Journal = {JAIR},
	Keywords = {{MDP}, {POMDP}},
	Owner = {skinathil},
	Pages = {99--134},
	Timestamp = {2014.05.23},
	Title = {{Planning and Acting in Partially Observable Stochastic Domains}},
	Url = {http://www.sciencedirect.com/science/article/pii/S000437029800023X},
	Volume = {101},
	Year = {1998},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S000437029800023X},
	Bdsk-Url-2 = {http://dx.doi.org/10.1016/S0004-3702(98)00023-X}}

@Article{Kalyanasundaram_AJC_2004,
  author =    {Kalyanasundaram, Suresh and Chong, Edwin K. P. and Shroff, Ness B.},
  title =     {Markov decision processes with uncertain transition rates: sensitivity and max hyphen min control},
  journal =   {Asian Journal of Control},
  year =      {2004},
  volume =    {6},
  number =    {2},
  pages =     {253--269},
  doi =       {10.1111/j.1934-6093.2004.tb00203.x},
  issn =      {1934-6093},
  keywords =  {Markov decision processes, sensitivity, max-min control, call admission control, policy iteration},
  publisher = {Blackwell Publishing Ltd},
  url =       {http://dx.doi.org/10.1111/j.1934-6093.2004.tb00203.x}
}

@Article{Perold_JPM_1988,
  author =        {Andr{\'e} F. Perold},
  title =         {The implementation shortfall: Paper versus reality},
  journal =       {The Journal of Portfolio Management},
  year =          {1988},
  volume =        {14},
  number =        {3},
  pages =         {4--9},
  date-added =    {2015-04-15 07:50:15 +0000},
  date-modified = {2015-04-15 07:52:46 +0000}
}

@inproceedings{Pineau_IJCAI_2003,
	Author = {Pineau, Joelle and Gordon, Geoffrey J. and Thrun, Sebastian},
	Booktitle = {IJCAI},
	Date-Added = {2015-11-10 12:03:08 +0000},
	Date-Modified = {2015-11-10 12:03:08 +0000},
	File = {Pineau_IJCAI_2003-PointBasedValueIterationAnytimeAlgorithmPOMDPs.pdf:C\:\\Users\\skinathil\\phd\\references\\Pineau_IJCAI_2003-PointBasedValueIterationAnytimeAlgorithmPOMDPs.pdf:application/pdf},
	Keywords = {Algorithm, {POMDP}},
	Pages = {1025--1030},
	Title = {{Point-based Value Iteration: An Anytime Algorithm for {POMDPs}}},
	Url = {http://dl.acm.org/citation.cfm?id=1630659.1630806},
	Year = {2003},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=1630659.1630806}}

@InProceedings{Reddy_IJCAI_2011,
  author =    {Reddy, Prashant P and Veloso, Manuela M},
  title =     {Strategy learning for autonomous agents in smart grid markets},
  booktitle = {IJCAI Proceedings-International Joint Conference on Artificial Intelligence},
  year =      {2011},
  volume =    {22},
  number =    {1},
  pages =     {1446}
}

@inproceedings{Sanner_UAI_2011,
	Author = {Sanner, Scott and Delgado, Karina and Nunes de Barros, Leliane},
	Booktitle = {UAI},
	Date-Added = {2016-01-19 04:48:21 +0000},
	Date-Modified = {2016-01-19 04:49:57 +0000},
	Pages = {1--10},
	Title = {{Symbolic Dynamic Programming for Discrete and Continuous State MDPs}},
	Year = {2011}}

@InCollection{Sutton_NIPS_1999,
  author =    {Sutton, Richard S and David A. McAllester and Satinder P. Singh and Mansour, Yishay},
  title =     {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
  booktitle = {Advances in Neural Information Processing Systems 12},
  publisher = {MIT Press},
  year =      {2000},
  editor =    {S.A. Solla and T.K. Leen and K. M\"{u}ller},
  pages =     {1057--1063},
  url =       {http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf}
}

@InCollection{Szita_RL_2012,
  author =    {Szita, Istv{\'a}n},
  title =     {Reinforcement learning in games},
  booktitle = {Reinforcement Learning},
  publisher = {Springer Berlin Heidelberg},
  year =      {2012},
  pages =     {539--577}
}

@Article{Tan_JAP_2011,
  author =    {Tan, Chin Hon and Hartman, Joseph C.},
  title =     {Sensitivity analysis in Markov decision processes with uncertain reward parameters},
  journal =   {J. Appl. Probab.},
  year =      {2011},
  volume =    {48},
  number =    {4},
  pages =     {954--967},
  month =     {12},
  doi =       {10.1239/jap/1324046012},
  fjournal =  {Journal of Applied Probability},
  publisher = {Applied Probability Trust},
  url =       {http://dx.doi.org/10.1239/jap/1324046012}
}

@article{Viswanathan_TIMS_1977,
	Author = {Viswanathan, B. and Aggarwal, V.V. and Nair, K.P.K.},
	Date-Added = {2015-11-10 10:37:18 +0000},
	Date-Modified = {2015-11-10 10:42:32 +0000},
	Journal = {TIMS Studies in the Management Sciences},
	Pages = {263--272},
	Title = {Multiple criteria Markov decision processes},
	Volume = {6},
	Year = {1977}}

@article{White_LSS_1980,
	Author = {White III, C. C. and Kim, K. M.},
	Date-Added = {2015-11-10 10:47:00 +0000},
	Date-Modified = {2015-11-24 04:51:48 +0000},
	Journal = {Large Scale Systems},
	Pages = {129--140},
	Title = {Solution Procedures for Solving Vector Criterion {Markov} Decision Processes},
	Volume = {1},
	Year = {1980}}

@Article{Williams_EM_2009,
  author =        {Byron K. Williams},
  title =         {Markov decision processes in natural resources management: Observability and uncertainty },
  journal =       {Ecological Modelling },
  year =          {2009},
  volume =        {220},
  number =        {6},
  pages =         {830 - 840},
  __markedentry = {[skin:]},
  abstract =      {The breadth and complexity of stochastic decision processes in natural resources presents a challenge to analysts who need to understand and use these approaches. The objective of this paper is to describe a class of decision processes that are germane to natural resources conservation and management, namely Markov decision processes, and to discuss applications and computing algorithms under different conditions of observability and uncertainty. A number of important similarities are developed in the framing and evaluation of different decision processes, which can be useful in their applications in natural resources management. The challenges attendant to partial observability are highlighted, and possible approaches for dealing with it are discussed. },
  doi =           {http://dx.doi.org/10.1016/j.ecolmodel.2008.12.023},
  issn =          {0304-3800},
  keywords =      {Natural resources},
  url =           {http://www.sciencedirect.com/science/article/pii/S0304380009000246}
}

@inproceedings{Zamani_AAAI_2012,
	Author = {Zamani, Zahra and Sanner, Scott},
	Booktitle = {AAAI},
	Date-Added = {2016-01-19 04:50:04 +0000},
	Date-Modified = {2016-01-19 04:50:56 +0000},
	Pages = {1--7},
	Title = {Symbolic Dynamic Programming for Continuous State and Action MDPs},
	Year = {2012}}

@Article{Zhu_AIM_2014,
  author =        {George Zhu and Dan Lizotte and Jesse Hoey},
  title =         {Scalable approximate policies for Markov decision process models of hospital elective admissions },
  journal =       {Artificial Intelligence in Medicine },
  year =          {2014},
  volume =        {61},
  number =        {1},
  pages =         {21 - 34},
  __markedentry = {[skin:6]},
  abstract =      {AbstractObjective To demonstrate the feasibility of using stochastic simulation methods for the solution of a large-scale Markov decision process model of on-line patient admissions scheduling. Methods The problem of admissions scheduling is modeled as a Markov decision process in which the states represent numbers of patients using each of a number of resources. We investigate current state-of-the-art real time planning methods to compute solutions to this Markov decision process. Due to the complexity of the model, traditional model-based planners are limited in scalability since they require an explicit enumeration of the model dynamics. To overcome this challenge, we apply sample-based planners along with efficient simulation techniques that given an initial start state, generate an action on-demand while avoiding portions of the model that are irrelevant to the start state. We also propose a novel variant of a popular sample-based planner that is particularly well suited to the elective admissions problem. Results Results show that the stochastic simulation methods allow for the problem size to be scaled by a factor of almost 10 in the action space, and exponentially in the state space. We have demonstrated our approach on a problem with 81 actions, four specialities and four treatment patterns, and shown that we can generate solutions that are near-optimal in about 100 s. Conclusion Sample-based planners are a viable alternative to state-based planners for large Markov decision process models of elective admissions scheduling. },
  doi =           {http://dx.doi.org/10.1016/j.artmed.2014.04.001},
  issn =          {0933-3657},
  keywords =      {Markov decision process},
  url =           {http://www.sciencedirect.com/science/article/pii/S0933365714000372}
}
