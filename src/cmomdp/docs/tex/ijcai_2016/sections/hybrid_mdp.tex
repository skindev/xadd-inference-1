\section{Parameterized Hybrid MDPs}
\label{sec:hybrid_mdps}

In this Section we introduce parameterized hybrid Markov Decision Processes (PHMDPs) and show how the framework can be specialized to investigate multi-objective rewards, parameter sensitivity and non-convex optimization of policy parameters. We also review their finite horizon solution via dynamic programming.

\subsection{Definition}
\label{sec:hybrid_mdps_def}

A parameterized hybrid Markov Decision Process (PHMDP) is defined by the tuple {\footnotesize \PMDPTuple}. {\footnotesize \State} specifies a vector of states given by {\footnotesize $(\vec{d}, \vec{x}) = \left( d_1, \ldots, d_m, x_1, \ldots, x_n \right) $}, where each {\footnotesize $ d_i \in \left\lbrace 0, 1 \right\rbrace $} {\footnotesize $\left( 1 \leq i \leq m \right)$} is boolean and each {\footnotesize$ x_j \in \Real $} {\footnotesize $\left( 1 \leq j \leq n \right)$} is continuous. {\footnotesize \Action} specifies a finite set of actions. PHMDPs
are naturally factored~\parencite{Boutilier_JAIR_1999} in terms of the state variables {\footnotesize$\vec{d}$, $\vec{x}$} and as such, the joint transition model can be written as:
{\footnotesize
\abovedisplayskip=0pt
\belowdisplayskip=0pt
\begin{align}
    \label{eq:hmdp_tfunc}
    \Transition :& \, \ProbArg{\vec{d}', \vec{x}' | \vec{d}, \vec{x}, a, \theta} = \nonumber \\
    & \prod_{i=1}^{m} \ProbArg{d_i' | \vec{d}, \vec{x}, a, \theta} \prod_{j=1}^{n} \ProbArg{x_j' | \vec{d}, \vec{d}', \vec{x}, a, \theta}.
\end{align}   
}

{\footnotesize \RewardFunc} is the reward function which encodes the preferences of the agent. {\footnotesize \Horizon} represents the number of decision steps until termination and the discount factor {\footnotesize $\gamma \in [0, 1)$} is used to geometrically discount future rewards. {\footnotesize $\theta \in \Theta$} is a free parameter from the parameter space {\footnotesize $ \Theta $}.

A policy {\footnotesize $\pi : \State \rightarrow \Action$}, specifies the action to take in every state. The value of a state {\footnotesize $(\vec{d}, \vec{x}) \in \State$} under a given policy {\footnotesize$\pi$} is given by:
{\footnotesize 
    \abovedisplayskip=0pt
    \belowdisplayskip=0pt
\begin{align*}
    V^{\pi}\left( \vec{d}, \vec{x}; \theta \right) &= \Exp{\sum\limits_{h = 0}^{\Horizon} \gamma^h \cdot r^h | \vec{d}, \vec{x}, \theta} \nonumber  
%    &= \sum\limits_{s' \in \State}  \Transition\left(s, \pi\left(s\right), s' \right) \left[ \Reward\left(s, \pi(s), s'\right) + \gamma \cdot V^{\pi}(s')\right]. \label{eq:mdp_bellman_eq}
\end{align*}
}
%Equation~\eqref{eq:mdp_bellman_eq} is known as the Bellman equation for $V^{\pi}$.

The state-action value function {\footnotesize$Q : \State \times \Action \rightarrow \mathbb{R}$} is given by:
{\footnotesize 
    \abovedisplayskip=0pt
    \belowdisplayskip=0pt
\begin{align*}
%    \label{eq:mdp_qfunc}
    Q^{\pi}\left(\vec{d}, \vec{x}, a; \theta\right) = \Exp{\sum\limits_{h = 0}^{\Horizon} \gamma^h \cdot r^h | \vec{d}, \vec{x}, a, \theta}.
\end{align*}
}

The optimal value function of policy {\footnotesize$ \pi^{*} $} satisfies
{\footnotesize 
\abovedisplayskip=0pt
\belowdisplayskip=0pt
\begin{align}
    \label{eq:opt_vfunc}
    V^{\pi^{*}}(\vec{d}, \vec{x}; \theta) &= \max_{a \in A} \left\{ Q^{\pi^{*}}(\vec{d}, \vec{x}, a; \theta) \right\}. 
\end{align}
}%

We note that in our formulation of parameterized hybrid MDPs the parameters {\footnotesize $ \theta $} are not learned, which is in contrast to the work of~\parencite{Dearden_UAI_1999}.

In the subsequent Sections we demonstrate how the PHMDP framework can be specialised into models capable of: (i) investigating multi-objective trade-offs; (ii) exact sensitivity analysis; and (iii) non-convex optimization of policy parameters.

\subsubsection{Multi-objective PHMDPs}

The PHMDP framework can be specialised into a multi-objective setting by specifying the reward as {\footnotesize \MORewardFunc} where {\footnotesize $ d $} is the dimension of the reward vector. In this formulation the parameter {\footnotesize $ \theta^{d} $}, specifies the preferences over the reward components.

\subsubsection{Sensitivity Analysis for PHMDPs}

Sensitivity analysis can be conducted within the PHMDP framework by specifying as value function as in Equation~\eqref{eq:opt_vfunc}. The value function can then be differentiated with respect to the parameter {\footnotesize $ \theta $}.

\subsubsection{Policy Gradient Methods for PHMDPs}

PHMDP policy parameters can be optimised by specifying the action {\footnotesize \Action} as {\footnotesize $ a(\theta) $}.

\subsection{Solution Methods}

Value iteration~\parencite{Bellman_PU_1957} is a general dynamic programming algorithm used to solve MDPs. We modify the algorithm to solve the parameterized hybrid MDP formulation presented in Section~\ref{sec:hybrid_mdps_def}. The key idea of VI is to successively approximate {\footnotesize $V^{\pi^{*}}(\vec{d}, \vec{x}; \theta)$} and {\footnotesize $Q^{\pi^{*}}(\vec{d}, \vec{x}, a; \theta)$} by {\footnotesize $V^{h}(\vec{d}, \vec{x}; \theta)$} and {\footnotesize $Q^{h}(\vec{d}, \vec{x}, a; \theta)$}, respectively, at each horizon {\footnotesize$h$}. These two functions satisfy the following recursive relationship:

{\footnotesize 
    \abovedisplayskip=0pt
    \belowdisplayskip=0pt
    \begin{align}
        Q^{h}(\vec{d}, \vec{x}, a; \theta) &= \Reward(\vec{d}, \vec{x}, a; \theta) + \gamma \cdot  \nonumber \\ 
        & \sum_{\vec{d}'} \int_{\vec{x}'} \ProbArg{\vec{d}', \vec{x}' | \vec{d}, \vec{x}, a; \theta} \cdot V^{h-1}(\vec{d}, \vec{x}; \theta) d\vec{x}' \label{eq:vi_qfunc} \\
        V^{h}(\vec{d}, \vec{x}; \theta) &= \max_{a \in A} \left\{ Q^{h}(\vec{d}, \vec{x}, a; \theta) \right\} \label{eq:vi_vfunc}
    \end{align}
}%

The algorithm can be executed by first initialising {\footnotesize $V^{0}(\vec{d}, \vec{x}; \theta)$}  to zero or the terminal reward. Then for each {\footnotesize$h$}, {\footnotesize $V^{h}(\vec{d}, \vec{x}; \theta)$} is calculated from {\footnotesize $V^{h-1}(\vec{d}, \vec{x}; \theta)$} via Equations~\eqref{eq:vi_qfunc} and~\eqref{eq:vi_vfunc}, until the intended $h$-stage-to-go value function is computed. 

%Value iteration converges linearly in the number of iterations to the true values of {\footnotesize $Q^{\pi^{*}}(\vec{d}, \vec{x}, a; \theta)$} and {\footnotesize $V^{\pi^{*}}(\vec{d}, \vec{x}; \theta)$}~\parencite{Bertsekas_1987}.

%One key challenge still remains, namely, dealing with the infinitely
%many states in $\vec{x}$. We know that the {\footnotesize $V^{h}(\vec{x})$} and 
%{\footnotesize $Q^{h}(\vec{x}, a_1, a_2)$} functions have structure, but are unable to derive
%them. Furthermore, given known structures for {\footnotesize $V^{h}(\vec{x})$} and 
%{\footnotesize $Q^{h}(\vec{x}, a_1, a_2)$} we must determine the restrictions that
%guarantee a tractable solution. The SDP framework in conjunction with its
%closed-form operations provide answers to both of these concerns.

In the next section we show that parameterized hybrid MDPs can be solved optimally for arbitrary horizons using symbolic dynamic programming.