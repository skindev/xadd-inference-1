\section{Related Work}
\label{sec:background}

%\begin{itemize}
%    \item MOMDPs - Pareto front methods (approximate), convex hull algorithm (exact) -- works for discrete parameterized MDPs, but not for factored or hybrid case -- we want to look at parameterized hybrid, no exact work in this space
%    \item Sensitivity analysis of MDPs -- but none exact in hybrid case
%    \item Policy gradient -- everything is numerically oriented, never a closed-form exact solution as a function of parameter, e.g., to analyze stability
%\end{itemize}

In this work, we present parameterized hybrid MDPs as a unifying framework which allows for multi-objective reasoning, exact sensitivity analysis and parametric policy gradient analysis. While there are no comparable frameworks which allow for the same breadth of functionality, we briefly survey prior art in each of these three areas.

The techniques used to solve multi-objective MDPs with unknown preferences depend on the nature of the scalarization function~\parencite{Roijers_JAIR_2013}. If the scalarization function is linear, then methods such as the Convex Hull Value Iteration algorithm~\parencite{Barrett_ICML_2008}, which returns the optimal policy for discrete multi-objective MDPs with any linear preference function, can be used. For non-linear scalarization functions the Pareto front of a set of undominated policies must be returned. The Pareto front can be prohibitively large and as a result solution techniques such as those of~\parencite{Chatterjee_STACS_2006} and~\parencite{Pirotta_AAAI_2015} have focussed on approximating the Pareto front, or using a refinement of Pareto dominance known as Lorenz optimality~\parencite{Perny_AAAI_2013} to further restrict the size of the solution set. In this work we present \textit{exact} multi-objective closed-form solutions to the more general class of \textit{parameterized hybrid} MDPs with factored states. 

Sensitivity analysis of parameters has focussed on uncertainty within the specification the transition function~\parencite{Kalyanasundaram_AJC_2004}, reward function~\parencite{Tan_JAP_2011, Hopp_JOTA_1988}, or a combination of both~\parencite{Givan_AI_2000}, in discrete MDPs. The framework that we introduce in this paper enables \textit{exact} sensitivity analysis for parameterized hybrid MDPs.

Policy gradient methods rely upon optimizing parameterized policies with respect to the expected return by gradient descent. The main concern of such methods is obtaining a good estimator of the policy gradient~\parencite{Peters_IRS_2006}. Two of the most prominent approaches have been the finite-difference methods such as those of~\parencite{Ng_UAI_2000} and likelihood methods such as~\parencite{Baxter_ISCAS_2000}, both of which are numerically oriented and sample based. Our contributions allow for policy gradients to be analysed as closed-form \textit{exact} solutions as a function of non-convex parameters.

%~\parencite{Sutton_NIPS_1999} \ldots