\begin{abstract}


It is often critical in real-world applications to: (i) perform inverse learning of the cost parameters of a multi-objective reward based on observed agent behavior; (ii) perform sensitivity analyses of policies to various parameter settings; and (iii) analyze and optimize policy performance as a function of policy parameters. When such problems have mixed discrete and continuous state and/or action spaces, this leads to parameterized hybrid MDPs (PHMDPs) that are often approximately solved via discretization, sampling, and/or local gradient methods (when optimization is involved). In this paper we combine two recent advances that allow for the first exact solution and optimization of PHMDPs. We first show how each of the aforementioned use cases can be formalized as PHMDPs, which can then be solved via an extension of symbolic dynamic programming (SDP) even when the solution is piecewise nonlinear. Secondly, we leverage recent advances in non-convex solvers such as dReal and dOp (that offer {\footnotesize $ \delta $}-optimality guarantees for nonlinear problems given a symbolic function) for non-convex global optimization in (i), (ii), and (iii) using SDP to derive symbolic solutions to each PHMDP formalization. We demonstrate the efficacy and scalability of our framework by calculating the first known exact solutions to complex nonlinear examples of each of the aforementioned use cases.

%Formalizing and solving MDPs in the presence of parameter uncertainty leads to a family of hybrid (mixed discrete and continuous state and action) value functions with nonlinear and/or piecewise structure. 
%In this paper we show how these models can be unified under the framework of Parameterized Hybrid MDPs (PHMDPs). 
%We also introduce a novel algorithm that combines PHMDPs and parameterized symbolic dynamic programming to derive nonlinear symbolic solutions to an array of challenging problems, including: (i) 
%inverse learning of parameters in multi-objective reward settings, sensitivity analysis and nonlinear parameterized policy optimization, all of which have no known exact solutions. 
%We then calculate exact solutions to the symbolic functions by leveraging advances in optimal non-convex solvers. 
%
%We demonstrate the efficacy of our framework by calculating the first known exact solutions to the following difficult nonlinear sequential decision optimization problems: (i) inverse learning of parameters of multi-objective rewards; (ii) nonlinear policy optimization for public health policies in epidemic models; and (iii) sensitivity analysis of trading strategies for portfolio transactions.

%Markov Decision Processes (MDPs) provide a powerful framework for decision-theoretic planning. While MDPs occur in many applications, their applicability is limited by the common assumption that the model parameters are known. However, there are many settings where it is important to solve or evaluate MDPs in the presence of uncertainty over parameters, for example: (i) to investigate the trade-off between different reward criteria in a multi-objective setting; (ii) to perform sensitivity analyses of policies to various parameter settings; and (iii) to analyse and optimise policy performance as a function of policy parameters. However, formalizing MDPs in this way leads to a family of hybrid (mixed discrete and continuous state and action) MDPs with non-linear and/or piecewise structure. In this paper we show how each of the aforementioned use cases can be formalized under the common framework of parameterized hybrid MDPs and solved exactly and in closed-form by leveraging techniques from symbolic dynamic programming. Our novel framework allows one to explore for the first-time: (i) an exact functional representation of the
%multi-objective trade-offs for use in (Interactive) Decision Maps; (ii) exact sensitivity analysis of public health policies in epidemic models over the full range of infection rate parameters; and (iii) non-convex optimization of policy parameters applied to finance problems previously impossible with sample-based policy gradient techniques.

\end{abstract}
