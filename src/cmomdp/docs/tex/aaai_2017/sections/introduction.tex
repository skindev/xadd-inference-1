\section{Introduction}
\label{sec:introduction}

Markov Decision Processes (MDPs)~\parencite{Howard_MIT_1960} are the de facto standard framework for decision theoretic planning in fully observable environments~\parencite{Boutilier_JAIR_1999}. MDPs occur in a wide range of real world domains such as game playing~\parencite{Szita_RL_2012}, power systems~\parencite{Reddy_IJCAI_2011}, ecology~\parencite{Williams_EM_2009} and patient admission scheduling~\parencite{Zhu_AIM_2014}. Traditional MDP solution techniques often assume that the parameters of the model are known. However, in practice, model parameters are usually estimated from limited data or elicited from humans and hence are naturally uncertain. It is often critical in real world applications to: (i) investigate the trade-off between different reward criteria in a multi-objective setting; (ii) perform sensitivity analyses of policies to various parameter settings; and (iii) analyze and optimize policy performance as a function of policy parameters.  Formalizing models to address each of the aforementioned use cases is often fraught, due to the specification leading to hybrid (mixed discrete and continuous state and/or action) MDPs with nonlinear and/or piecewise structure that have been traditionally very difficult to solve.

In this paper we make the following key contributions:
\begin{itemize}
\item We present the unifying framework of {\it Parameterized Hybrid MDPs} (PHMDPs) which enables the investigation of trade-offs between multi-objective reward criteria, parameter sensitivity analysis and non-convex analysis and optimization of policy parameters.

\item We provide an algorithm that solves this class of PHMDPs exactly and in closed-form by defining a parameterized variant of Symbolic Dynamic Programming (SDP)~\parencite{Boutilier_IJCAI_2001} extended to hybrid MDPs~\parencite{Sanner_UAI_2011}. By leveraging SDP we are able to define nonlinear symbolic objects that can be optimized optimally by state-of-the-art non-convex optimizers such as dReal~\parencite{Gao2013}.

\item We use the PHMDP framework in conjunction with parameterized SDP and state-of-the-art non-convex optimizers to calculate the first exact solutions to: (i) inverse learning of the parameters of a multi-objective reward; (ii) non-convex optimization of public health policies in epidemic models; and (iii) exact sensitivity analyses of trading strategies for portfolio transactions. 
\end{itemize}

We remark that prior to this paper, it was an open question as to whether all of the above use cases admitted closed-form solutions --- these questions are resolved affirmatively through PHMDPs and SDP.