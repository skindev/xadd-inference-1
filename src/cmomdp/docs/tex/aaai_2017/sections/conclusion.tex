\section{Conclusions and Future Work}
\label{sec:conclusion}

In this paper we introduced Parameterized Hybrid MDPs as a unifying framework which enables the investigation of trade-offs between multi-objective reward criteria, the examination of parameter sensitivity and the non-convex optimization of continuous policy parameters. We have also presented a novel parametric extension of symbolic dynamic programming which enable PHMDPs to be expressed as symbolic objects which may then be used as an input into a state-of-the-art non-convex optimizer. We demonstrated the utility of our framework and algorithm by calculating the first known exact and closed-form solutions to the inverse learning of parameters of a multi-objective reward, non-convex optimization of public health policies and sensitivity analysis of trading strategies for portfolio transactions.

There are a number of avenues for future research. Firstly, it is important to examine more general representations of the reward and transition functions while still guaranteeing exact solutions. Another direction of research lies within improving the scalability of the algorithm by either extending techniques for Algebraic Decision Diagrams~\parencite{Bahar_JoFMiSD_1993} from APRICODD~\parencite{St-Aubin_NIPS_2000} under the current restrictions on the reward and transition functions, bounded error compression for 
XADDs~\parencite{Vianna_UAI_2013} for more expressive representations, or lazy approximation of value functions as piecewise linear XADDs~\parencite{Li_AAAI_2005}. The advances made within this paper open up a number of potential novel research paths which may be used to progress multi-objective analyses, sensitivity analyses and nonlinear parameterized policy optimization for difficult nonlinear sequential decision making problems.
