\section{Parameterized Hybrid MDPs}
\label{sec:hybrid_mdps}

In this section we introduce Parameterized Hybrid Markov Decision Processes (PHMDPs).

\subsection{Definition}
\label{sec:phmdp_def}

A parameterized hybrid Markov Decision Process (PHMDP) is defined by the tuple {\footnotesize \PMDPTuple}. {\footnotesize \State} specifies a vector of states given by {\footnotesize $(\vec{d}, \vec{x}) =  \left( d_1, \ldots, d_m, x_1, \ldots, x_n \right) $}, where each {\footnotesize $ d_i \in \left\lbrace 0, 1 \right\rbrace $} {\footnotesize $\left( 1 \leq i \leq m \right)$} is discrete and each {\footnotesize$ x_j \in \Real $} {\footnotesize $\left( 1 \leq j \leq   n \right)$} is continuous. {\footnotesize $\Action^{h}_{s}$} specifies a finite set of state and horizon dependent actions.  {\footnotesize $\vec{\theta} $} are free parameters from the parameter space {\footnotesize $ \Theta $}. PHMDPs are naturally factored~\cite{Boutilier_JAIR_1999} in terms of the state variables {\footnotesize$\vec{d}$} and {\footnotesize
$\vec{x}$}. Hence, the joint transition model can be written as:
{\footnotesize
\abovedisplayskip=0pt
\belowdisplayskip=0pt
\begin{align}
    \label{eq:hmdp_tfunc}
    \Transition :& \, \ProbArg{ \left. \vec{d}', \vec{x}' \middle| \vec{d}, \vec{x}, a, \vec{\theta} \right.} = \nonumber \\
    & \prod_{i=1}^{m} \ProbArg{ \left. d_i' \middle| \vec{d}, \vec{x}, a, \vec{\theta} \right. } \prod_{j=1}^{n} \ProbArg{ \left. x_j' \middle| \vec{d}, \vec{d}', \vec{x}, a, \vec{\theta} \right. },
\end{align}   
}
where {\footnotesize $ a \in \Action^{h}_{s} $}. The transition model permits discrete noise in the sense that {\footnotesize $ \ProbArg{ x_j' | \vec{d}, \vec{d}', \vec{x}, a, \vec{\theta} } $} may condition on $ \vec{d}' $, which are stochastically sampled according to their conditional probability functions. We present the transition model and factorization as a simple Dynamic Bayesian Network (DBN) without synchronic arcs for ease of exposition but note this framework can be extended to DBNs with arbitrary intermediate variable layers that allow one to emulate synchronic arc dependencies and relax the discrete/continuous stratification.

{\footnotesize \RewardFunc} is the reward function which encodes the preferences of the agent. {\footnotesize \Horizon} represents the number of decision steps until termination and the discount factor {\footnotesize $\gamma \in [0, 1)$} is used to geometrically discount future rewards. A policy {\footnotesize $\pi : \State \times \Horizon \times \vec{\theta} \rightarrow \Action$}, specifies the action to take in every state and horizon. The value function of the optimal policy {\footnotesize$ \pi^{*} $} satisfies:
{\footnotesize 
    \abovedisplayskip=0pt
    \belowdisplayskip=0pt
    \begin{align}
        \label{eq:opt_vfunc}
        V^{\pi^{*}}\left(\vec{d}, \vec{x}; \vec{\theta}\right) &= \max_{a \in \Action} \left\{ Q^{\pi}\left(\vec{d}, \vec{x}, a; \vec{\theta}\right) \right\}.
    \end{align}
}%
{\footnotesize $ Q^{\pi}\left(\vec{d}, \vec{x}, a; \vec{\theta}\right) $} gives the expected return starting from state {\footnotesize $(\vec{d}, \vec{x}) \in \State$}, taking action {\footnotesize $ a \in \Action^{h}_{s} $}, and then following policy $ \pi $. In general, an agent's objective is to find an optimal policy {\footnotesize$ \pi^{*} $} which maximises the expected sum of discounted rewards over horizon {\footnotesize \Horizon} \footnote{All of the code can be found online at \url{https://github.com/skindev/xadd-inference-1/src/cmomdp}.}. We again remark that in our formulation of PHMDPs the parameters {\footnotesize $ \vec{\theta} $} are free parameters and not learned from reward and transition samples.

In subsequent sections we demonstrate how the PHMDP framework can be specialized into models capable of: (i) investigating multi-objective reward criteria; (ii) exact parameter sensitivity analysis; and (iii) optimization of continuous non-convex policy parameters.