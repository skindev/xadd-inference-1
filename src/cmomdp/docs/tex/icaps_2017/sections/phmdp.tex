\section{Parameterized Hybrid MDPs}
\label{sec:hybrid_mdps}

In this section we introduce Parameterized Hybrid Markov Decision Processes (PHMDPs) and show how the framework can be specialized into models capable of: (i) investigating multi-objective reward criteria; (ii) exact parameter sensitivity analysis.; and (iii) optimization of continuous non-convex policy parameters. 
%We also review their finite horizon solution via dynamic programming.

\subsection{Definition}
\label{sec:phmdp_def}

A parameterized hybrid Markov Decision Process (PHMDP) is defined by the tuple {\footnotesize \PMDPTuple}. {\footnotesize \State} specifies a vector of states given by {\footnotesize $(\vec{d}, \vec{x}) =  \left( d_1, \ldots, d_m, x_1, \ldots, x_n \right) $}, where each {\footnotesize $ d_i \in \left\lbrace 0, 1 \right\rbrace $} {\footnotesize $\left( 1 \leq i \leq m \right)$} is discrete and each {\footnotesize$ x_j \in \Real $} {\footnotesize $\left( 1 \leq j \leq   n \right)$} is continuous. {\footnotesize $\Action^{h}_{s}$} specifies a finite set of state and horizon dependent actions.  {\footnotesize $\vec{\theta} \in \Theta$} are free parameters from the parameter space {\footnotesize $ \Theta $}. PHMDPs are naturally factored~\parencite{Boutilier_JAIR_1999} in terms of the state variables {\footnotesize$\vec{d}$} and {\footnotesize
$\vec{x}$}. Hence, the joint transition model can be written as:
{\footnotesize
\abovedisplayskip=0pt
\belowdisplayskip=0pt
\begin{align}
    \label{eq:hmdp_tfunc}
    \Transition :& \, \ProbArg{ \left. \vec{d}', \vec{x}' \middle| \vec{d}, \vec{x}, a, \vec{\theta} \right.} = \nonumber \\
    & \prod_{i=1}^{m} \ProbArg{ \left. d_i' \middle| \vec{d}, \vec{x}, a, \vec{\theta} \right. } \prod_{j=1}^{n} \ProbArg{ \left. x_j' \middle| \vec{d}, \vec{d}', \vec{x}, a, \vec{\theta} \right. },
\end{align}   
}
where {\footnotesize $ a \in \Action^{h}_{s} $}. The transition model permits discrete noise in the sense that {\footnotesize $ \ProbArg{ x_j' | \vec{d}, \vec{d}', \vec{x}, a, \vec{\theta} } $} may condition on $ \vec{d}' $, which are stochastically sampled according to their conditional probability functions. 
%It is clear that while the transition model does not permit general stochastic transition noise, they do permit discrete noise in the sense that {\footnotesize $ \ProbArg{ x_j' | \vec{d}, \vec{d}', \vec{x}, a, \vec{\theta} } $} may condition on $ \vec{d}' $, which are stochastically sampled according to their conditional probability functions. 
%We refer the reader to~\parencite{Zamani_AAAI_2012,Sanner_UAI_2011} for further details.

{\footnotesize \RewardFunc} is the reward function which encodes the preferences of the agent. {\footnotesize \Horizon} represents the number of decision steps until termination and the discount factor {\footnotesize $\gamma \in [0, 1)$} is used to geometrically discount future rewards. A policy {\footnotesize $\pi : \State \times \Horizon \rightarrow \Action$}, specifies the action to take in every state and horizon. The value function of the optimal policy {\footnotesize$ \pi^{*} $} satisfies:
{\footnotesize 
    \abovedisplayskip=0pt
    \belowdisplayskip=0pt
    \begin{align}
        \label{eq:opt_vfunc}
        V^{\pi^{*}}\left(\vec{d}, \vec{x}; \vec{\theta}\right) &= \max_{a \in \Action} \left\{ Q^{\pi}\left(\vec{d}, \vec{x}, a; \vec{\theta}\right) \right\}.
    \end{align}
}%
{\footnotesize $ Q^{\pi}\left(\vec{d}, \vec{x}, a; \vec{\theta}\right) $} gives the expected return starting from state {\footnotesize $(\vec{d}, \vec{x}) \in \State$}, taking action {\footnotesize $ a \in \Action^{h}_{s} $}, and then following policy $ \pi $. In general, an agent's objective is to find an optimal policy {\footnotesize$ \pi^{*} $} which maximises the expected sum of discounted rewards over horizon {\footnotesize \Horizon}.



%The state-value function {\footnotesize $ V^{\pi}(\vec{d}, \vec{x}; \vec{\theta}) $} is the expected return starting from state {\footnotesize $(\vec{d}, \vec{x}) \in \State$}, and then following policy $ \pi $:
%{\footnotesize
%    \abovedisplayskip=0pt
%    \belowdisplayskip=0pt
%\begin{align*}
%    V^{\pi}\left( \vec{d}, \vec{x}; \vec{\theta} \right) &= \Exp{ \left. \sum\limits_{h = 0}^{\Horizon} \gamma^h \cdot r^h \middle| \vec{d}, \vec{x}, \vec{\theta}, \pi \right.}. \nonumber  
%%    &= \sum\limits_{s' \in \State}  \Transition\left(s, \pi\left(s\right), s' \right) \left[ \Reward\left(s, \pi(s), s'\right) + \gamma \cdot V^{\pi}(s')\right]. \label{eq:mdp_bellman_eq}
%\end{align*}
%}
%%Equation~\eqref{eq:mdp_bellman_eq} is known as the Bellman equation for $V^{\pi}$.
%Here, {\footnotesize $r^h$} is the reward obtained at horizon $ h $ following the policy $ \pi $ where we assume a starting state of {\footnotesize $ (\vec{d}, \vec{x}) $} at {\footnotesize $ h = 0 $}.
%
%The state-action value function {\footnotesize $Q : \State \times \Action \rightarrow \mathbb{R}$} is the expected return starting from state {\footnotesize $(\vec{d}, \vec{x}) \in \State$}, taking action {\footnotesize $ a \in \Action^{h}_{s} $}, and then following policy $ \pi $:
%{\footnotesize 
%    \abovedisplayskip=0pt
%    \belowdisplayskip=0pt
%\begin{align}
%    \label{eq:mdp_qfunc}
%    Q^{\pi}\left(\vec{d}, \vec{x}, a; \vec{\theta}\right) = \Exp{ \left. \sum\limits_{h = 0}^{\Horizon} \gamma^h \cdot r^h \middle| \vec{d}, \vec{x}, a, \vec{\theta}, \pi \right.}. 
%\end{align}
%}

We again remark that in our formulation of PHMDPs the parameters {\footnotesize $ \vec{\theta} $} are free parameters and not learned from reward and transition samples.
%, as for example in Bayesian reinforcement learning, as discussed in section~\ref{sec:background}. 

%In subsequent sections we demonstrate how the PHMDP framework can be specialized into models capable of: (i) investigating multi-objective reward criteria; (ii) exact parameter sensitivity analysis; and (iii) optimization of continuous non-convex policy parameters.

\subsubsection{Inverse Learning for Multi-objective PHMDPs}

The PHMDP framework can be specialized into a multi-objective setting by specifying the reward as {\footnotesize \MORewardFunc}, where {\footnotesize $ d $} is the dimension of the reward vector. In this formulation the parameter {\footnotesize $ \vec{\theta}^{d} $} specifies the linear scalarization over the reward components (i.e, {\footnotesize $\langle \Reward, \vec{\theta}^{d} \rangle$}).
% and the value function can be expressed as a function of all possible scalarizations. 
%Multi-objective PHMDPs can also be used for the inverse learning of the parameters of the multi-objective reward that explains observed behavior. A more formal definition is provide in section~\ref{sec:sdp}. 
%An example of this inverse reinforcement learning inspired analysis is shown in section~\ref{sec:results_navigation}.

\subsubsection{Sensitivity Analysis for PHMDPs}

To conduct a sensitivity analysis, we assume that {\footnotesize $\vec{\theta}^{d}$} are unknown parameters and we solve the value function in Equation~\eqref{eq:opt_vfunc} as a parametric function, {\footnotesize $V^{\pi^{*}}(\vec{d}, \vec{x}; \vec{\theta}^{d})$}, of these parameters. The parametric value function can then be analyzed under the optimal policy {\footnotesize $ \pi^{*} $} as a function of other state variables {\footnotesize $(\vec{d}, \vec{x})$}.
%The parametric value function can then be displayed or analyzed to allow a user to explore trade-offs between parameters {\footnotesize $\vec{\theta}^{d}$} or the optimal policy {\footnotesize $ \pi^{*} $} as a function of other state variables {\footnotesize $(\vec{d}, \vec{x})$}. 
%An example of sensitivity analysis is shown in section~\ref{sec:results_oe}.

\subsubsection{Nonlinear Parameterized Policy Optimization Methods for PHMDPs}

PHMDP policy parameters can be analyzed and optimized by parameterizing a policy {\footnotesize $\pi$} by action parameters {\footnotesize $\vec{\theta}^{d}$}, and evaluating the value function with respect to the parameterized policy. The resulting non-convex policy value, as a function of {\footnotesize $\vec{\theta}^{d}$}, can be analyzed and optimized using the parametric form of {\footnotesize $V^{\pi}(\vec{d}, \vec{x}; \vec{\theta}^{d})$} and all symbolic derivatives, up to any order, of this value function. Symbolic derivatives avoid the stochasticity issues associated with traditional policy gradient methods.
%An example of nonlinear parameterized policy optimization is shown in section~\ref{sec:results_influenza}. 

%\subsection{Solution Methods}
%Value iteration (VI)~\parencite{Bellman_PU_1957} can be modified to solve PHMDPs as follows:
%Value iteration (VI)~\parencite{Bellman_PU_1957} is a general dynamic programming algorithm used to solve Markov Decision Processes (MDPs). In this section we modify VI to solve PHMDPs. The key idea of VI is to successively approximate {\footnotesize $V^{\pi^{*}}(\vec{d}, \vec{x}; \vec{\theta})$} and {\footnotesize $Q^{\pi^{*}}(\vec{d}, \vec{x}, a; \vec{\theta})$} by {\footnotesize $V^{h}(\vec{d}, \vec{x}; \vec{\theta})$} and
%{\footnotesize $Q^{h}(\vec{d}, \vec{x}, a; \vec{\theta})$}, respectively, at each horizon {\footnotesize$h \in \Horizon$}. These two functions satisfy the following recursive relationship:
%{\footnotesize 
%    \abovedisplayskip=0pt
%    \belowdisplayskip=0pt
%    \begin{align}
%        Q^{h} \left( \vec{d}, \vec{x}, a; \vec{\theta} \right) = \Reward \left( \vec{d}, \vec{x}, a; \vec{\theta} \right) + \gamma \, \cdot &  \nonumber \\ 
%        \sum_{\vec{d}'} \int_{\vec{x}'} \ProbArg{\left. \vec{d}', \vec{x}' \middle| \vec{d}, \vec{x}, a; \vec{\theta} \right.} \cdot V^{h-1}\left( \vec{d}', \vec{x}'; \vec{\theta} \right) & \, d\vec{x}'  \label{eq:vi_qfunc} \quad \\
%        V^{h}\left( \vec{d}, \vec{x}; \vec{\theta} \right) = \max_{a \in A} \left\{ Q^{h}\left( \vec{d}, \vec{x}, a; \vec{\theta} \right) \right\} & \label{eq:vi_vfunc}
%    \end{align}
%}%
%
%{\footnotesize $\ProbArg{\left. \vec{d}', \vec{x}' \middle| \vec{d}, \vec{x}, a; \vec{\theta} \right. }$ } is specified in Equation~\eqref{eq:hmdp_tfunc}. 
%The algorithm can be executed by first initialising {\footnotesize   $V^{0}(\vec{d}, \vec{x}; \vec{\theta})$} to zero or the terminal reward. Then for each {\footnotesize$h$}, {\footnotesize $V^{h}(\vec{d}, \vec{x}; \vec{\theta})$} is calculated from {\footnotesize $V^{h-1}(\vec{d}, \vec{x}; \vec{\theta})$} via Equations~\eqref{eq:vi_qfunc} and~\eqref{eq:vi_vfunc}, until the intended $h$-stage-to-go value function is computed.
%
%%Despite the ease with which VI can be modified to solve PHMDPs, 
%Despite the ease of modification, two key challenges still remain: (i) dealing with infinitely many states in {\footnotesize $\vec{x}$} and {\footnotesize $\vec{\theta}$}; and (ii) determining restrictions on {\footnotesize $Q^{h}(\vec{d}, \vec{x}, a; \vec{\theta})$} and {\footnotesize $V^{h}(\vec{d}, \vec{x}; \vec{\theta})$} that guarantee tractable solutions. 
%In the next section we show that a parameterized extension of symbolic dynamic programming can be used to address these concerns and solve PHMDPs, optimally and in closed-form, for arbitrary horizons.