\section{Related Work}
\label{sec:background}

In this section we briefly survey prior art in the areas of multi-objective reasoning, exact sensitivity analysis and nonlinear parameterized policy optimization and conclude with a discussion of alternate uses of the term \emph{parameterized} in the MDP literature that contrasts with our work.

The techniques used to solve Multi-objective MDPs (MOMDPs) with unknown preferences depend on the nature of the scalarization function used to weight each reward component~\cite{Roijers_JAIR_2013}. Methods such as the Convex Hull Value Iteration algorithm~\cite{Barrett_ICML_2008} can be used for discrete \emph{enumerated state} MOMDPs with any linear preference function. Nonlinear scalarization functions require the calculation of the Pareto front, which can be prohibitively large. As a result, Pareto front approximation techniques such as those of ~\cite{Chatterjee_STACS_2006} and~\cite{Pirotta_AAAI_2015} or Lorenz optimal refinements such as~\cite{Perny_AAAI_2013} are often used. In this work we present the first exact \emph{factored hybrid} MOMDP solutions as a symbolic function of multiobjective weights via the framework of PHMDPs and SDP.

To date, most research into sensitivity analysis of MDP parameters has focused on uncertainty within the specification of the transition function~\cite{Kalyanasundaram_AJC_2004}, reward function~\cite{Tan_JAP_2011}, or a combination of both~\cite{Givan_AI_2000}, in discrete MDPs. The framework that we introduce in this paper enables \textit{exact} sensitivity analysis for PHMDPs that allows it to be applied in continuous state settings and permits the derivation and analysis of the \emph{optimal} policy as a symbolic function of these parameters.

Policy gradient methods rely upon optimizing parameterized policies with respect to the expected return by gradient descent. 
Two of the most prominent approaches have been the finite-difference methods, such as those of~\cite{Ng_UAI_2000}, and Monte Carlo methods, such as~\cite{Sutton_NIPS_1999,Baxter_ISCAS_2000}, both of which only converge to local optima.
%are numerically oriented and sample based. 
Our use of PHMDPs and SDP allows us to solve for a globally optimal policy as a parameterized function of policy parameters.

Finally, as a point of differentiation from other uses of the term \emph{parameterized} in the MDP literature, we remark that other works~\cite{Doshi-VelezK16,Duff_UMA_2002,Dearden_UAI_1999,Gopalan_COLT_2015} have used Parameterized MDP to refer to MDPs with latent parameters whose beliefs can be updated by observing reward and transition samples. In contrast, in this work we assume strict uncertainty of continuous MDP parameters in models that are otherwise fully specified; in this way we can treat parameters simply as free variables that can be parametrically analyzed via recent advances in symbolic solution methods and non-convex optimizers~\cite{Gao2013}.


